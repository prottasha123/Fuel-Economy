# -*- coding: utf-8 -*-
"""Fuel_Economy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18WXt0GGHIfzl-46UisvIY58hj4sJY1kY
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
file_path = '/content/auto-mpg.csv'
df = pd.read_csv(file_path)

df.head()

# 1. Check datatypes and columns for missing or unusual values
print("Data Types and Info:")
print(df.info())

# 1. Check datatypes and columns for missing or unusual values

print("\nMissing Values Count:")
print(df.isnull().sum())

# Checking for unusual values
print("\nFirst few rows of the dataset to inspect data:")
print(df.head())

# 2. Convert 'origin' to a categorical feature
df['origin'] = df['origin'].astype('object')
print("\nUnique values in 'origin':", df['origin'].unique())
df.info()

df.query("horsepower == '?' ")

df["horsepower"] = pd.to_numeric(df["horsepower"], errors='coerce')
df.info()

df["horsepower"] = df["horsepower"].fillna(df["horsepower"].mean())
df.info()

# 3. Calculate summary statistics for each numeric column
numeric_summary = df.describe()
print("\nSummary Statistics for Numeric Columns:")
print(numeric_summary)

# Build a histogram for the target variable ('mpg')
plt.figure(figsize=(8, 5))
plt.hist(df['mpg'], bins=13, color='lightblue', edgecolor='black')
plt.title('Histogram of MPG')
plt.xlabel('MPG')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# 4. Explore relationships between features and 'mpg'
# Scatterplots for numerical features vs 'mpg'
numeric_columns = df.select_dtypes(include=np.number).columns
plt.figure(figsize=(12, 10))
for i, col in enumerate(numeric_columns):
    if col != 'mpg':
        plt.subplot(3, 3, i + 1)
        plt.scatter(df[col], df['mpg'], alpha=0.7, color='green')
        plt.title(f'MPG vs {col}')
        plt.xlabel(col)
        plt.ylabel('MPG')
plt.tight_layout()
plt.show()

sns.barplot(data=df,x="origin",y="mpg")
plt.show()

df.corr(numeric_only=True)

# Correlation heatmap
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Identify the column most strongly correlated with 'mpg'
correlation_matrix = df.corr(numeric_only=True) # Assign the correlation matrix to the variable
most_correlated = correlation_matrix['mpg'].abs().sort_values(ascending=False)
# Identify the column most strongly correlated with 'mpg'
correlation_matrix = df.corr(numeric_only=True) # Assign the correlation matrix to the variable
most_correlated = correlation_matrix['mpg'].abs().sort_values(ascending=False)
print("\nMost Strongly Correlated Columns with 'mpg':")
print(most_correlated[1:])  # Exclude 'mpg' itself
# Use code with caution

df_model = df.assign(
    weight2  = df["weight"] ** 2
).drop(columns=["car name"],axis=1)

df_model = pd.get_dummies(df_model,drop_first=True)

df_model.head()

from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.metrics import r2_score as r2, mean_absolute_error as mae
import statsmodels.api as sm

# Define features as a list of strings, not a tuple of lists and strings
features = ["weight", "weight2", "cylinders", "displacement", "horsepower", "weight", "acceleration", "model year", "origin_2", "origin_3"]
X = sm.add_constant(df_model[features])  # Assign the features to X, not y
y = df["mpg"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=2023)  # Use X instead of x

kf = KFold(n_splits=5, shuffle=True, random_state=2023)

# Create a list to store validation scores for each fold
cv_lm_r2s = []
cv_lm_mae = []

# Loop through each fold in X and y
for train_ind, val_ind in kf.split(X, y):
    # Subset data based on cv folds
    X_train, y_train = X.iloc[train_ind], y.iloc[train_ind]
    X_val, y_val = X.iloc[val_ind], y.iloc[val_ind]
    # Fit the Model on fold's training data
    model = sm.OLS(y_train, X_train).fit()
    # Append Validation score to list
    cv_lm_r2s.append(r2(y_val, model.predict(X_val),))
    cv_lm_mae.append(mae(y_val, model.predict(X_val),))
kf = KFold(n_splits=5, shuffle=True, random_state=2023)

# Create a list to store validation scores for each fold
cv_lm_r2s = []
cv_lm_mae = []

# Loop through each fold in X and y
for train_ind, val_ind in kf.split(X, y):
    # Subset data based on cv folds
    X_train, y_train = X.iloc[train_ind], y.iloc[train_ind]
    X_val, y_val = X.iloc[val_ind], y.iloc[val_ind]
    # Fit the Model on fold's training data
    model = sm.OLS(y_train, X_train).fit()
    # Append Validation score to list
    cv_lm_r2s.append(r2(y_val, model.predict(X_val)))
    cv_lm_mae.append(mae(y_val, model.predict(X_val)))

print("All Validation R2s: ", [round(x, 3) for x in cv_lm_r2s])
print(f"Cross Val R2s : {round(np.mean(cv_lm_r2s),3)} +- {round(np.std(cv_lm_r2s),3)}")


print("All Validation MAEs: ", [round(x, 3) for x in cv_lm_mae])
print(f"Cross Val MAEs : {round(np.mean(cv_lm_mae),3)} +- {round(np.std(cv_lm_mae),3)}")

def residual_analysis_plots(model):
    import scipy.stats as stats
    import matplotlib.pyplot as plt

    predictions = model.predict()
    residuals = model.resid

    fig, ax = plt.subplots(1, 2, sharey="all", figsize=(10, 6))

    sns.scatterplot(x=predictions, y=residuals, ax=ax[0])
    ax[0].set_title("Residual Plot")
    ax[0].set_ylabel("Residuals")

    stats.probplot(residuals, dist="norm", plot=ax[1])
    ax[1].set_title("Normal Q-Q Plot")

residual_analysis_plots(model)

model.summary()

# Convert all columns in X_train and y_train to numeric types
X_train = X_train.astype(float)
y_train = y_train.astype(float)

# Now fit the model
model = sm.OLS(y_train, X_train).fit()  # Fit model with training data

# Now predict using this model
print(f"Test R2: {r2(y_test, model.predict(X_test)),}")
print(f"Test MAE: {mae(y_test, model.predict(X_test)),}")

from sklearn.preprocessing import StandardScaler

std = StandardScaler()
x_m = std.fit_transform(X.values)
x_te = std.transform(X_test.values)

from sklearn.linear_model import RidgeCV

n_alphas = 200
alphas = 10 ** np.linspace(-3, 3, n_alphas)
ridge_model = RidgeCV(alphas=alphas, cv=5)

# Change X_m to x_m to match the variable defined in the previous cell
ridge_model.fit(x_m, y)

print(ridge_model.score(x_m, y))
print(mae(y, ridge_model.predict(x_m)))
print(ridge_model.alpha_)

# Calculate the correlation between 'weight' and 'mpg'
correlation_weight_mpg = df['weight'].corr(df['mpg'])
round(correlation_weight_mpg,2)